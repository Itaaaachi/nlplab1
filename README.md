# nlplab1
分词实验
分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符，虽然英文也同样存在短语的划分问题，不过在词这一层上，中文比之英文要复杂得多、困难得多。当前中文分词的主要困难在于歧义识别和新词识别。
当前的分词方法主要分为三类：第一类为基于字符串匹配的分词方法，比如正向最大匹配法（由左到右的方向）、逆向最大匹配法（由右到左的方向）、最少切分（使每一句中切出的词数最小）和双向最大匹配法（进行由左到右、由右到左两次扫描）等；第二类为基于理解的分词方法，即过让计算机模拟人对句子的理解，达到识别词的效果；最后一类为基于统计语言模型的分词方法，主要统计模型有N元文法模型、隐马尔可夫模型（HMM）、最大熵模型（ME）和条件随机场模型（CRF）等。本次实验中，包括如下内容：
词典的构建
正反向最大匹配分词实现
正反向最大匹配分词效果分析
基于机械匹配的分词系统的速度优化
基于统计语言模型的分词系统实现
